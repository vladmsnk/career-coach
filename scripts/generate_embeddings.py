#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª.
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python scripts/generate_embeddings.py
"""
import asyncio
import os
import pickle
import json
import re
import math
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

import numpy as np
import pandas as pd
from tenacity import retry, wait_exponential_jitter, stop_after_attempt, retry_if_exception_type
from openai import AsyncOpenAI, APIError, RateLimitError, APITimeoutError
import tiktoken

# ==================== –ù–ê–°–¢–†–û–ô–ö–ò ====================
MODEL = "text-embedding-3-small"
DIM = 768  # 1536 –º–∞–∫—Å–∏–º—É–º; 768 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
CONCURRENCY = 2  # –£–ú–ï–ù–¨–®–ï–ù–û –¥–ª—è –æ–±—Ö–æ–¥–∞ —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–æ–≤
BATCH_SIZE = 25   # –£–ú–ï–ù–¨–®–ï–ù–û –¥–ª—è –æ–±—Ö–æ–¥–∞ —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–æ–≤
DELAY_BETWEEN_BATCHES = 2  # –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

PICKLE_FILE = "scored_vacs.pickle"
OUTPUT_FILE = "vacancies_with_embeddings.pickle"

# ==================== EMBEDDING –§–£–ù–ö–¶–ò–ò ====================
enc = tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
    return len(enc.encode(text or ""))

def chunk_by_tokens(text: str, max_tokens: int = 400, overlap: int = 40) -> List[str]:
    """–†–µ–∑–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º."""
    ids = enc.encode(text or "")
    if not ids:
        return [""]
    chunks, step = [], max_tokens - overlap
    for i in range(0, len(ids), step):
        sub = ids[i:i+max_tokens]
        if not sub: 
            break
        chunks.append(enc.decode(sub))
    return chunks or [""]

@retry(
    wait=wait_exponential_jitter(initial=2, max=60),
    stop=stop_after_attempt(10),
    retry=retry_if_exception_type((RateLimitError, APITimeoutError, APIError))
)
async def embed_batch(client: AsyncOpenAI, texts: List[str], *, dimensions: int = DIM) -> np.ndarray:
    """–û–¥–∏–Ω –±–∞—Ç—á –≤ Embeddings API —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Ä–µ—Ç—Ä–∞—è–º–∏."""
    try:
        resp = await client.embeddings.create(
            model=MODEL,
            input=texts,
            dimensions=dimensions
        )
        vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
        # L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥ cosine/IP
        norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12
        return vecs / norms
    except RateLimitError as e:
        print(f"‚ö†Ô∏è  –†–µ–π—Ç-–ª–∏–º–∏—Ç! –ü–æ–¥–æ–∂–¥–µ–º –¥–æ–ª—å—à–µ... {e}")
        raise
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ embed_batch: {e}")
        raise

async def embed_long_docs(docs: List[str]) -> np.ndarray:
    """–î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã: —á–∞–Ω–∫–∏–Ω–≥ (~400 —Ç–æ–∫–µ–Ω–æ–≤) + mean-pooling –≤ 1 –≤–µ–∫—Ç–æ—Ä –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç."""
    if not docs:
        print("‚ö†Ô∏è  –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!")
        return np.array([]).reshape(0, DIM)
    
    client = AsyncOpenAI()
    sem = asyncio.Semaphore(CONCURRENCY)

    print(f"üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞...")
    print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: CONCURRENCY={CONCURRENCY}, BATCH_SIZE={BATCH_SIZE}, DELAY={DELAY_BETWEEN_BATCHES}s")

    # 1) –≥–æ—Ç–æ–≤–∏–º –∫—É—Å–∫–∏ –∏ –æ–±—Ä–∞—Ç–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    doc_spans: List[str] = []
    doc_ptrs: List[int] = []  # —Å–∫–æ–ª—å–∫–æ —Å–ø–∞–Ω–æ–≤ —É i-–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for text in docs:
        spans = chunk_by_tokens(text, max_tokens=400, overlap=40)
        doc_spans.extend(spans)
        doc_ptrs.append(len(spans))

    print(f"üìä –í—Å–µ–≥–æ —Å–ø–∞–Ω–æ–≤: {len(doc_spans)}")
    
    if not doc_spans:
        print("‚ö†Ô∏è  –ù–µ—Ç —Å–ø–∞–Ω–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return np.array([]).reshape(0, DIM)

    # 2) –±–∞—Ç—á–∏–Ω–≥ —Å–ø–∞–Ω–æ–≤
    async def run_batch(start: int, end: int, batch_num: int):
        async with sem:
            if batch_num > 0:
                await asyncio.sleep(DELAY_BETWEEN_BATCHES)
            batch_texts = doc_spans[start:end]
            print(f"   üîÑ –ë–∞—Ç—á {batch_num + 1}: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∞–Ω—ã {start}-{end} ({len(batch_texts)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
            return await embed_batch(client, batch_texts)

    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –±–∞—Ç—á–µ–π
    tasks = []
    for batch_num, start in enumerate(range(0, len(doc_spans), BATCH_SIZE)):
        end = min(start + BATCH_SIZE, len(doc_spans))
        tasks.append(run_batch(start, end, batch_num))

    # –í—ã–ø–æ–ª–Ω—è–µ–º –±–∞—Ç—á–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º concurrency
    print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º {len(tasks)} –±–∞—Ç—á–µ–π...")
    try:
        span_vecs_list = await asyncio.gather(*tasks)
        span_vecs = np.vstack(span_vecs_list)
        print(f"‚úÖ –ü–æ–ª—É—á–∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Å–ø–∞–Ω–æ–≤: {span_vecs.shape}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–µ–π: {e}")
        raise

    # 3) —Å–æ–±–∏—Ä–∞–µ–º —Å–ø–∞–Ω—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã (mean-pooling)
    doc_vecs = []
    ptr = 0
    for num_spans in doc_ptrs:
        if num_spans == 0:
            print("‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç –±–µ–∑ —Å–ø–∞–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä")
            doc_vecs.append(np.zeros(DIM, dtype=np.float32))
        else:
            chunk_embeddings = span_vecs[ptr:ptr+num_spans]
            pooled = chunk_embeddings.mean(axis=0)
            doc_vecs.append(pooled)
        ptr += num_spans

    result = np.array(doc_vecs, dtype=np.float32)
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result.shape}")
    return result

def parse_vacancy_from_answers(answers_list) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ answers_list."""
    try:
        if not answers_list or len(answers_list) == 0:
            return {}
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –æ—Ç–≤–µ—Ç (–æ–±—ã—á–Ω–æ —Ç–∞–º –≤–∞–∫–∞–Ω—Å–∏—è)
        answer = answers_list[0]
        if not isinstance(answer, str):
            return {}
        
        # –£–±–∏—Ä–∞–µ–º markdown —Ä–∞–∑–º–µ—Ç–∫—É –∏ –ø–∞—Ä—Å–∏–º JSON
        clean_answer = answer.strip()
        if clean_answer.startswith('```'):
            lines = clean_answer.split('\n')
            # –ò—â–µ–º JSON –º–µ–∂–¥—É ```
            json_start = None
            json_end = None
            for i, line in enumerate(lines):
                if line.strip() == '{':
                    json_start = i
                elif line.strip() == '```' and json_start is not None:
                    json_end = i
                    break
            
            if json_start is not None and json_end is not None:
                json_str = '\n'.join(lines[json_start:json_end])
                return json.loads(json_str)
        
        return {}
    
    except Exception as e:
        return {}

def prepare_task_text(row) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞."""
    try:
        # –ü–∞—Ä—Å–∏–º –≤–∞–∫–∞–Ω—Å–∏—é –∏–∑ answers_list
        vacancy_data = parse_vacancy_from_answers(row.get('answers_list', []))
        
        if not vacancy_data:
            return ""
        
        tasks = vacancy_data.get('tasks', [])
        if not tasks:
            return ""
            
        # –ï—Å–ª–∏ tasks —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ —Ç–µ–∫—Å—Ç
        if isinstance(tasks, list):
            return " ".join(str(task).strip() for task in tasks if str(task).strip())
        else:
            return str(tasks).strip()
    
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á: {e}")
        return ""

def expand_vacancy_data(df: pd.DataFrame) -> pd.DataFrame:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç DataFrame, –ø–∞—Ä—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ answers_list."""
    print(f"üîÑ –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ answers_list...")
    
    expanded_rows = []
    for _, row in df.iterrows():
        vacancy_data = parse_vacancy_from_answers(row.get('answers_list', []))
        
        if vacancy_data:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–º–∏
            expanded_row = {
                'id': row.get('id'),
                'hh_id': str(vacancy_data.get('url', '').split('/')[-1]) if vacancy_data.get('url') else str(row.get('id', '')),
                'answers_list': row.get('answers_list'),
                'dialog': row.get('dialog'),
                **vacancy_data  # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø–æ–ª—è –∏–∑ vacancy_data
            }
            expanded_rows.append(expanded_row)
        else:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å vacancy –¥–ª—è ID {row.get('id')}")
    
    expanded_df = pd.DataFrame(expanded_rows)
    print(f"‚úÖ –†–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(expanded_df)} –≤–∞–∫–∞–Ω—Å–∏–π")
    return expanded_df

def clean_and_validate_data(df: pd.DataFrame) -> pd.DataFrame:
    """–û—á–∏—â–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""
    print(f"üìä –ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"üìã –ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö: {list(df.columns)}")
    
    # –°–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—à–∏—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ, –ø–∞—Ä—Å—è answers_list
    df = expand_vacancy_data(df)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ hh_id –∏–ª–∏ id
    initial_len = len(df)
    if 'hh_id' in df.columns:
        df = df.drop_duplicates(subset=['hh_id'], keep='first').reset_index(drop=True)
        if len(df) < initial_len:
            print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {initial_len - len(df)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ hh_id")
    elif 'id' in df.columns:
        df = df.drop_duplicates(subset=['id'], keep='first').reset_index(drop=True)
        if len(df) < initial_len:
            print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {initial_len - len(df)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ id")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∑–∞–¥–∞—á
    print("üîÑ –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –∑–∞–¥–∞—á...")
    df['tasks_text'] = df.apply(prepare_task_text, axis=1)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏
    df_with_tasks = df[df['tasks_text'].str.len() > 0].reset_index(drop=True)
    
    removed_count = len(df) - len(df_with_tasks)
    if removed_count > 0:
        print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {removed_count} –∑–∞–ø–∏—Å–µ–π –±–µ–∑ –∑–∞–¥–∞—á")
    
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(df_with_tasks)}")
    return df_with_tasks

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    print("üöÄ –ì–ï–ù–ï–†–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –î–õ–Ø –í–ê–ö–ê–ù–°–ò–ô")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if not Path(PICKLE_FILE).exists():
        print(f"‚ùå –§–∞–π–ª {PICKLE_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª scored_vacs.pickle –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞")
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º OpenAI API –∫–ª—é—á
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è: export OPENAI_API_KEY='your-key'")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {PICKLE_FILE}...")
    try:
        with open(PICKLE_FILE, "rb") as f:
            data = pickle.load(f)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DataFrame –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(data, list):
            print(f"üìã –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ DataFrame...")
            df = pd.DataFrame(data)
        elif isinstance(data, pd.DataFrame):
            df = data
        else:
            print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(data)}")
            return
            
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        return
    
    # –û—á–∏—â–∞–µ–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    df = clean_and_validate_data(df)
    
    if len(df) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
        return
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    print("üîÑ –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞...")
    texts = df['tasks_text'].tolist()
    
    # –°—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å
    total_tokens = sum(count_tokens(text) for text in texts)
    estimated_cost = (total_tokens / 1_000_000) * 0.02  # $0.02 per 1M tokens
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(texts):,}")
    print(f"   üî§ –¢–æ–∫–µ–Ω–æ–≤: {total_tokens:,}")
    print(f"   üí∞ –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${estimated_cost:.2f}")
    print(f"   ü§ñ –ú–æ–¥–µ–ª—å: {MODEL}")
    print(f"   üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {DIM}")
    
    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    confirmation = input(f"\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤? (y/N): ").strip().lower()
    if confirmation not in ['y', 'yes']:
        print("‚õî –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
        return
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    print("\nüîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    start_time = datetime.now()
    
    try:
        embeddings = await embed_long_docs(texts)
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ: {embeddings.shape}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return
    
    # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫ –¥–∞–Ω–Ω—ã–º
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    result_data = {
        'dataframe': df,
        'embeddings': embeddings,
        'metadata': {
            'model': MODEL,
            'dimensions': DIM,
            'created_at': datetime.now().isoformat(),
            'total_records': len(df),
            'total_tokens': total_tokens,
            'estimated_cost': estimated_cost
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    try:
        with open(OUTPUT_FILE, "wb") as f:
            pickle.dump(result_data, f)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {OUTPUT_FILE}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    duration = datetime.now() - start_time
    print(f"\nüéâ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"   ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration}")
    print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(df):,}")
    print(f"   ü§ñ –°–æ–∑–¥–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings):,}")
    print(f"   üíæ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {OUTPUT_FILE}")
    print(f"   üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {Path(OUTPUT_FILE).stat().st_size / 1024 / 1024:.1f} MB")

if __name__ == "__main__":
    asyncio.run(main())
