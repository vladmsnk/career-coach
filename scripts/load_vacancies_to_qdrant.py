#!/usr/bin/env python3
"""
Pipeline –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π –≤ Qdrant –Ω–∞ –æ—Å–Ω–æ–≤–µ vladosy.ipynb
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python scripts/load_vacancies_to_qdrant.py
"""
import asyncio
import os
import pickle
import json
import re
import math
from pathlib import Path
from typing import List, Dict, Any, Optional
from uuid import UUID, uuid5, NAMESPACE_URL
from datetime import datetime

import numpy as np
import pandas as pd
from tenacity import retry, wait_exponential_jitter, stop_after_attempt, retry_if_exception_type
from openai import AsyncOpenAI, APIError, RateLimitError, APITimeoutError
import tiktoken

from qdrant_client import QdrantClient
from qdrant_client.http.models import (
    VectorParams, Distance, PointStruct, 
    CollectionStatus, OptimizersConfigDiff
)

# ==================== –ù–ê–°–¢–†–û–ô–ö–ò ====================
MODEL = "text-embedding-3-small"
DIM = 768  # 1536 –º–∞–∫—Å–∏–º—É–º; 768 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
CONCURRENCY = 2  # –£–ú–ï–ù–¨–®–ï–ù–û: –±—ã–ª–æ 8 ‚Üí —Å—Ç–∞–ª–æ 2 –¥–ª—è –æ–±—Ö–æ–¥–∞ —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–æ–≤
BATCH_SIZE = 25   # –£–ú–ï–ù–¨–®–ï–ù–û: –±—ã–ª–æ 128 ‚Üí —Å—Ç–∞–ª–æ 25 –¥–ª—è –æ–±—Ö–æ–¥–∞ —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–æ–≤
DELAY_BETWEEN_BATCHES = 2  # –î–û–ë–ê–í–õ–ï–ù–û: –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "vacancies_tasks"
PICKLE_FILE = "scored_vacs.pickle"

# ==================== EMBEDDING –§–£–ù–ö–¶–ò–ò ====================
enc = tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ."""
    return len(enc.encode(text or ""))

def chunk_by_tokens(text: str, max_tokens: int = 400, overlap: int = 40) -> List[str]:
    """–†–µ–∑–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º."""
    ids = enc.encode(text or "")
    if not ids:
        return [""]
    chunks, step = [], max_tokens - overlap
    for i in range(0, len(ids), step):
        sub = ids[i:i+max_tokens]
        if not sub: 
            break
        chunks.append(enc.decode(sub))
    return chunks or [""]

@retry(
    wait=wait_exponential_jitter(initial=2, max=60),  # –£–í–ï–õ–ò–ß–ï–ù–û: –±–æ–ª—å—à–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ —Ä–µ—Ç—Ä–∞—è—Ö
    stop=stop_after_attempt(10),  # –£–í–ï–õ–ò–ß–ï–ù–û: –±–æ–ª—å—à–µ –ø–æ–ø—ã—Ç–æ–∫
    retry=retry_if_exception_type((RateLimitError, APITimeoutError, APIError))
)
async def embed_batch(client: AsyncOpenAI, texts: List[str], *, dimensions: int = DIM) -> np.ndarray:
    """–û–¥–∏–Ω –±–∞—Ç—á –≤ Embeddings API —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Ä–µ—Ç—Ä–∞—è–º–∏."""
    try:
        resp = await client.embeddings.create(
            model=MODEL,
            input=texts,
            dimensions=dimensions
        )
        vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
        # L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥ cosine/IP
        norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12
        return vecs / norms
    except RateLimitError as e:
        print(f"‚ö†Ô∏è  –†–µ–π—Ç-–ª–∏–º–∏—Ç! –ü–æ–¥–æ–∂–¥–µ–º –¥–æ–ª—å—à–µ... {e}")
        raise
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ embed_batch: {e}")
        raise

async def embed_long_docs(docs: List[str]) -> np.ndarray:
    """–î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã: —á–∞–Ω–∫–∏–Ω–≥ (~400 —Ç–æ–∫–µ–Ω–æ–≤) + mean-pooling –≤ 1 –≤–µ–∫—Ç–æ—Ä –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç."""
    if not docs:
        print("‚ö†Ô∏è  –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!")
        return np.array([]).reshape(0, DIM)
    
    client = AsyncOpenAI()
    sem = asyncio.Semaphore(CONCURRENCY)

    print(f"üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞...")
    print(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: CONCURRENCY={CONCURRENCY}, BATCH_SIZE={BATCH_SIZE}, DELAY={DELAY_BETWEEN_BATCHES}s")

    # 1) –≥–æ—Ç–æ–≤–∏–º –∫—É—Å–∫–∏ –∏ –æ–±—Ä–∞—Ç–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    doc_spans: List[str] = []
    doc_ptrs: List[int] = []  # —Å–∫–æ–ª—å–∫–æ —Å–ø–∞–Ω–æ–≤ —É i-–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for text in docs:
        spans = chunk_by_tokens(text, max_tokens=400, overlap=40)
        doc_spans.extend(spans)
        doc_ptrs.append(len(spans))

    print(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(doc_spans)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    if not doc_spans:
        print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏!")
        return np.array([]).reshape(0, DIM)

    # 2) —ç–º–±–µ–¥–¥–∏–º —Å–ø–∞–Ω—ã –±–∞—Ç—á–∞–º–∏ —Å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
    async def run_batch(start: int, end: int, batch_num: int):
        async with sem:
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
            if batch_num > 0:
                delay = DELAY_BETWEEN_BATCHES + (batch_num % 5) * 0.5  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –∫–∞–∂–¥—ã–µ 5 –±–∞—Ç—á–µ–π
                print(f"‚è∏Ô∏è  –ü–∞—É–∑–∞ {delay:.1f}s –ø–µ—Ä–µ–¥ –±–∞—Ç—á–µ–º #{batch_num}...")
                await asyncio.sleep(delay)
            
            batch = doc_spans[start:end]
            print(f"ü§ñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á #{batch_num}: {len(batch)} —Ç–µ–∫—Å—Ç–æ–≤")
            return await embed_batch(client, batch)

    tasks = []
    batch_num = 0
    for i in range(0, len(doc_spans), BATCH_SIZE):
        end_idx = min(i + BATCH_SIZE, len(doc_spans))
        tasks.append(asyncio.create_task(run_batch(i, end_idx, batch_num)))
        batch_num += 1
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5 –±–∞—Ç—á–µ–π (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞)
        if len(tasks) % 5 == 0:
            print(f"‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –±–∞—Ç—á–µ–π: {len(tasks)}")
    
    print(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(tasks)} –±–∞—Ç—á–µ–π –≤ OpenAI API —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–µ–π—Ç-–ª–∏–º–∏—Ç–æ–≤...")
    print(f"‚ö° –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{len(tasks) * DELAY_BETWEEN_BATCHES / 60:.1f} –º–∏–Ω—É—Ç")
    span_vecs = np.vstack(await asyncio.gather(*tasks))
    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(span_vecs)} —á–∞–Ω–∫–æ–≤")

    # 3) mean-pooling –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    out = np.zeros((len(docs), span_vecs.shape[1]), dtype=np.float32)
    idx = 0
    for doc_id, cnt in enumerate(doc_ptrs):
        if cnt > 0:
            out[doc_id] = span_vecs[idx:idx+cnt].mean(axis=0)
        idx += cnt
    
    # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    norms = np.linalg.norm(out, axis=1, keepdims=True) + 1e-12
    return out / norms

# ==================== –ü–ê–†–°–ò–ù–ì PICKLE ====================
def _extract_json_from_md(s: str) -> Optional[Dict[str, Any]]:
    """–í answers_list –ª–µ–∂–∏—Ç —Å—Ç—Ä–æ–∫–∞ —Å JSON, —á–∞—Å—Ç–æ –≤–Ω—É—Ç—Ä–∏ ``` ... ```."""
    if not isinstance(s, str):
        return None
    m = re.search(r"```(?:json)?\s*(.*?)\s*```", s, flags=re.S)
    payload = m.group(1) if m else s
    try:
        return json.loads(payload)
    except json.JSONDecodeError:
        cleaned = payload.replace("\u00a0", " ").strip()
        try:
            return json.loads(cleaned)
        except Exception:
            return None

def _join_tasks(tasks: Any, max_chars: int = 8000) -> str:
    """–°–∫–ª–µ–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞."""
    if isinstance(tasks, list):
        parts = [t.strip() for t in tasks if isinstance(t, str) and t.strip()]
        return (". ".join(parts))[:max_chars]
    return ""

def parse_pickle_to_tasks(pickle_path: str) -> pd.DataFrame:
    """–ü–∞—Ä—Å–∏—Ç pickle —Ñ–∞–π–ª –≤ DataFrame —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏."""
    print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ {pickle_path}...")
    
    if not Path(pickle_path).exists():
        raise FileNotFoundError(f"–§–∞–π–ª {pickle_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞.")
    
    with open(pickle_path, "rb") as f:
        rows = pickle.load(f)

    print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –∏–∑ pickle: {len(rows)}")

    out: List[Dict[str, Any]] = []
    processed = 0
    
    for row in rows:
        processed += 1
        if processed % 1000 == 0:
            print(f"‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {processed}/{len(rows)}")
            
        top_id = row.get("id")
        answers = row.get("answers_list") or []
        parsed: Optional[Dict[str, Any]] = None

        # –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π JSON –∏–∑ answers_list
        for ans in answers:
            parsed = _extract_json_from_md(ans)
            if isinstance(parsed, dict) and parsed:
                break
        if not parsed:
            continue

        url = parsed.get("url")
        # –≤—ã—Ç–∞—â–∏–º hh id –∏–∑ URL (–µ—Å–ª–∏ –µ—Å—Ç—å)
        m = re.search(r"/vacancy/(\d+)", url or "")
        hh_id = m.group(1) if m else None

        tasks_list = parsed.get("tasks") or []
        tasks_text = _join_tasks(tasks_list)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞–¥–∞—á
        if not tasks_text.strip():
            continue

        out.append({
            "id": top_id,
            "hh_id": hh_id,
            "url": url,
            "title": parsed.get("title"),
            "company": parsed.get("company"),
            "location": parsed.get("location"),
            "experience": parsed.get("experience"),
            "employment_type": parsed.get("employment_type"),
            "remote": parsed.get("remote"),
            "posted_at": parsed.get("posted_at"),
            "tasks_list": tasks_list,
            "tasks_text": tasks_text,
            "skills": parsed.get("skills") or [],
            "raw_category": parsed.get("category"),
            "confidence": parsed.get("confidence"),
        })

    df = pd.DataFrame(out)
    # —É–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã id
    if not df.empty:
        df = df.drop_duplicates(subset=["id"]).reset_index(drop=True)
    
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {len(df)}")
    return df

# ==================== QDRANT –§–£–ù–ö–¶–ò–ò ====================
def to_qdrant_id(x):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç ID –≤ —Ñ–æ—Ä–º–∞—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è Qdrant."""
    s = str(x)
    if s.isdigit() and len(s) < 19:  # int64 –ª–∏–º–∏—Ç
        return int(s)
    try:
        return UUID(s)
    except Exception:
        return uuid5(NAMESPACE_URL, s)

def to_jsonable(v):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç."""
    if isinstance(v, np.generic):
        return v.item()
    if isinstance(v, (list, tuple, np.ndarray)):
        return [to_jsonable(e) for e in v]
    if isinstance(v, (pd.Timestamp, datetime)):
        return v.isoformat()
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass
    return v

async def setup_qdrant_collection(client: QdrantClient, collection_name: str, vector_dim: int):
    """–°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant."""
    print(f"üîÑ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    try:
        collection_info = client.get_collection(collection_name)
        print(f"‚ö†Ô∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º...")
        client.delete_collection(collection_name)
    except Exception:
        print(f"‚ÑπÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é...")

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
    client.create_collection(
        collection_name=collection_name,
        vectors_config={
            "tasks": VectorParams(size=vector_dim, distance=Distance.COSINE)
        },
        optimizers_config=OptimizersConfigDiff(
            default_segment_number=2,
            memmap_threshold=20000
        )
    )
    print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–∑–¥–∞–Ω–∞ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {vector_dim})")

async def upload_to_qdrant(
    client: QdrantClient, 
    collection_name: str,
    df: pd.DataFrame,
    embeddings: np.ndarray,
    batch_size: int = 256
):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ Qdrant."""
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ {len(df)} –≤–∞–∫–∞–Ω—Å–∏–π –≤ Qdrant...")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ metadata –ø–æ–ª–µ–π
    relevant_cols = ["url", "title", "company", "location", "remote",
                    "employment_type", "experience", "posted_at",
                    "skills", "tasks_list", "raw_category", "confidence",
                    "hh_id"]
    
    points = []
    uploaded = 0
    errors = 0
    
    for idx, row in df.iterrows():
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ payload
            payload = {}
            for col in relevant_cols:
                if col in df.columns:
                    val = row[col]
                    if isinstance(val, np.ndarray):
                        val = val.tolist()
                    payload[col] = to_jsonable(val)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ point
            point_id = to_qdrant_id(row["id"])
            vector = embeddings[idx].tolist()
            
            point = PointStruct(
                id=point_id,
                vector={"tasks": vector},
                payload=payload
            )
            points.append(point)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞—Ç—á–∞–º–∏
            if len(points) >= batch_size:
                client.upsert(collection_name=collection_name, points=points)
                uploaded += len(points)
                print(f"üì§ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {uploaded}/{len(df)} –≤–∞–∫–∞–Ω—Å–∏–π...")
                points.clear()
                
        except Exception as e:
            errors += 1
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–∏ {row.get('id', 'unknown')}: {e}")
            if errors > 100:  # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫
                print(f"‚ùå –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ ({errors}). –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É.")
                break
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞—Ç–æ–∫
    if points:
        client.upsert(collection_name=collection_name, points=points)
        uploaded += len(points)
    
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ Qdrant: {uploaded} –≤–∞–∫–∞–Ω—Å–∏–π")
    if errors > 0:
        print(f"‚ö†Ô∏è –û—à–∏–±–æ–∫ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {errors}")

async def test_search(client: QdrantClient, collection_name: str):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –≤ —Å–æ–∑–¥–∞–Ω–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
    print(f"\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}...")
    
    test_queries = [
        "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ –Ω–∞ Python —Å FastAPI –∏ PostgreSQL",
        "Frontend —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ React –∏ TypeScript",
        "Java —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ —Å –æ–ø—ã—Ç–æ–º Spring Boot",
        "DevOps –∏–Ω–∂–µ–Ω–µ—Ä —Å Docker –∏ Kubernetes"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nüîç –¢–µ—Å—Ç {i}: {query}")
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = (await embed_long_docs([query]))[0].tolist()
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏
        search_results = client.search(
            collection_name=collection_name,
            query_vector=("tasks", query_embedding),
            limit=3,
            with_payload=True
        )
        
        if search_results:
            for j, result in enumerate(search_results, 1):
                payload = result.payload or {}
                title = payload.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
                company = payload.get('company', '–ë–µ–∑ –∫–æ–º–ø–∞–Ω–∏–∏')
                category = payload.get('raw_category', '–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
                print(f"   {j}. {title} | {company} | {category} | Score: {result.score:.3f}")
        else:
            print("   –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

# ==================== –û–°–ù–û–í–ù–û–ô PIPELINE ====================
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è pipeline."""
    start_time = datetime.now()
    print("üöÄ –ó–ê–ü–£–°–ö PIPELINE –ó–ê–ì–†–£–ó–ö–ò –í–ê–ö–ê–ù–°–ò–ô –í QDRANT")
    print("=" * 70)
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    print("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
    
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë —á–µ—Ä–µ–∑: export OPENAI_API_KEY='your-key-here'")
        print("   –ò–ª–∏ —Å–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º: OPENAI_API_KEY=your-key-here")
        return
    else:
        print("‚úÖ OpenAI API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω")
    
    if not Path(PICKLE_FILE).exists():
        print(f"‚ùå –§–∞–π–ª {PICKLE_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        print(f"   –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –µ–≥–æ –≤: {Path.cwd() / PICKLE_FILE}")
        return
    else:
        print(f"‚úÖ –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω: {PICKLE_FILE}")
    
    # 2. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant
    print(f"\nüîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant: {QDRANT_URL}")
    try:
        qdrant_client = QdrantClient(url=QDRANT_URL)
        collections = qdrant_client.get_collections()
        print(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω. –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π: {len(collections.collections)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        if collections.collections:
            print("   –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
            for coll in collections.collections:
                print(f"     ‚Ä¢ {coll.name}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Qdrant –∑–∞–ø—É—â–µ–Ω: docker-compose up -d qdrant")
        return
    
    # 3. –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìä –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {PICKLE_FILE}...")
    try:
        df = parse_pickle_to_tasks(PICKLE_FILE)
        if df.empty:
            print("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(df):,}")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {df['raw_category'].nunique()}")
    
    # –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    top_categories = df['raw_category'].value_counts().head(5)
    print(f"   –¢–æ–ø-5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
    for cat, count in top_categories.items():
        print(f"     ‚Ä¢ {cat}: {count:,}")
    
    # –¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–∏
    top_companies = df['company'].value_counts().head(5)
    print(f"   –¢–æ–ø-5 –∫–æ–º–ø–∞–Ω–∏–π:")
    for company, count in top_companies.items():
        if pd.notna(company):
            print(f"     ‚Ä¢ {company}: {count:,}")
    
    # 4. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print(f"\nü§ñ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    print(f"   –ú–æ–¥–µ–ª—å: {MODEL}")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {DIM}")
    print(f"   –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {CONCURRENCY}")
    print(f"   –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {BATCH_SIZE}")
    
    texts = df["tasks_text"].astype(str).tolist()
    
    # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
    print(f"\nüí∞ –ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏...")
    sample_texts = texts[:100] if len(texts) > 100 else texts
    sample_tokens = [count_tokens(text) for text in sample_texts]
    avg_tokens = sum(sample_tokens) / len(sample_tokens)
    total_tokens = int(avg_tokens * len(texts))
    estimated_cost = (total_tokens / 1_000_000) * 0.02  # $0.02 per 1M tokens –¥–ª—è text-embedding-3-small
    
    print(f"   –°—Ä–µ–¥–Ω–µ–µ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é: {avg_tokens:.0f}")
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens:,}")
    print(f"   –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${estimated_cost:.2f}")
    
    # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –∑–∞–ø—É—Å–∫–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å input)
    print(f"\n‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ! –≠—Ç–æ —Å–æ–∑–¥–∞—Å—Ç {len(texts):,} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞ ~${estimated_cost:.2f}")
    
    try:
        embeddings = await embed_long_docs(texts)
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ: {embeddings.shape}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return
    
    # 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant
    print(f"\nüóÑÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant...")
    try:
        await setup_qdrant_collection(qdrant_client, COLLECTION_NAME, DIM)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
        return
    
    # 6. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant
    print(f"\nüì§ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Qdrant...")
    try:
        await upload_to_qdrant(qdrant_client, COLLECTION_NAME, df, embeddings)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Qdrant: {e}")
        return
    
    # 7. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    try:
        collection_info = qdrant_client.get_collection(COLLECTION_NAME)
        points_count = qdrant_client.count(COLLECTION_NAME).count
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
        print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {COLLECTION_NAME}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫: {points_count:,}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {DIM}")
        print(f"   –°—Ç–∞—Ç—É—Å: {collection_info.status}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
    
    # 8. –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    try:
        await test_search(qdrant_client, COLLECTION_NAME)
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–∏—Å–∫–∞: {e}")
    
    # 9. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print(f"\nüéâ PIPELINE –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print("=" * 70)
    print(f"üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.1f} —Å–µ–∫—É–Ω–¥ ({duration/60:.1f} –º–∏–Ω—É—Ç)")
    print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(df):,}")
    print(f"   ü§ñ –°–æ–∑–¥–∞–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(embeddings):,}")
    print(f"   üóÑÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è Qdrant: {COLLECTION_NAME}")
    print(f"   üåê URL Qdrant Dashboard: http://localhost:6333/dashboard")
    print(f"   üîç URL Qdrant API: {QDRANT_URL}")
    
    print(f"\nüéØ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print(f"   1. –û—Ç–∫—Ä–æ–π—Ç–µ Qdrant Dashboard: http://localhost:6333/dashboard")
    print(f"   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–ª–ª–µ–∫—Ü–∏—é '{COLLECTION_NAME}'")
    print(f"   3. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –ø–æ–∏—Å–∫ –≤ –≤–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ")
    
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏ —Å–æ–∑–¥–∞–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
